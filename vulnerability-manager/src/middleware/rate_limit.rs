use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use uuid::Uuid;

#[derive(Clone, Debug)]
pub struct RateLimitConfig {
    pub admin_requests_per_minute: u32,
    pub team_leader_requests_per_minute: u32,
    pub user_requests_per_minute: u32,
}

impl Default for RateLimitConfig {
    fn default() -> Self {
        Self {
            admin_requests_per_minute: 1000,
            team_leader_requests_per_minute: 500,
            user_requests_per_minute: 100,
        }
    }
}

#[derive(Clone)]
struct UserRateLimitState {
    last_requests: Arc<RwLock<HashMap<Uuid, Vec<Instant>>>>,
}

impl UserRateLimitState {
    fn new() -> Self {
        Self {
            last_requests: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    async fn check_and_update(&self, user_id: Uuid, limit: u32, window: Duration) -> bool {
        let now = Instant::now();
        let window_start = now - window;

        let mut requests = self.last_requests.write().await;
        let user_requests = requests.entry(user_id).or_insert_with(Vec::new);

        // Remove old requests outside the window
        user_requests.retain(|&req_time| req_time > window_start);

        // Check if we've exceeded the limit
        if user_requests.len() >= limit as usize {
            return false;
        }

        // Add the current request
        user_requests.push(now);
        true
    }

    async fn cleanup_old_entries(&self) {
        let now = Instant::now();
        let cutoff = now - Duration::from_secs(3600); // Keep 1 hour of history

        let mut requests = self.last_requests.write().await;
        requests.retain(|_, times| times.iter().any(|&t| t > cutoff));
    }
}

#[derive(Clone)]
pub struct RateLimiter {
    state: UserRateLimitState,
    config: RateLimitConfig,
}

impl RateLimiter {
    pub fn new(config: RateLimitConfig) -> Self {
        Self {
            state: UserRateLimitState::new(),
            config,
        }
    }

    pub async fn check(&self, user_id: Uuid, role: &str) -> bool {
        let limit = match role {
            "admin" => self.config.admin_requests_per_minute,
            "team_leader" => self.config.team_leader_requests_per_minute,
            _ => self.config.user_requests_per_minute,
        };
        self.state.check_and_update(user_id, limit, Duration::from_secs(60)).await
    }
}

pub fn rate_limit_middleware(config: RateLimitConfig) {
    // Placeholder for compatibility
}

pub fn create_rate_limit_middleware(config: RateLimitConfig) -> RateLimiter {
    let limiter = RateLimiter::new(config);
    let limiter_clone = limiter.clone_state();

    // Spawn cleanup task
    tokio::spawn(async move {
        let mut interval = tokio::time::interval(Duration::from_secs(300));
        loop {
            interval.tick().await;
            limiter_clone.cleanup_old_entries().await;
        }
    });

    limiter
}

pub fn create_rate_limiter_layer(config: RateLimitConfig) -> RateLimiter {
    create_rate_limit_middleware(config)
}

pub fn spawn_cleanup_task(_state: Arc<RwLock<HashMap<Uuid, Vec<Instant>>>>) {
    // Cleanup is already spawned in create_rate_limit_middleware
}

impl RateLimiter {
    pub fn clone_state(&self) -> UserRateLimitState {
        self.state.clone()
    }
}
